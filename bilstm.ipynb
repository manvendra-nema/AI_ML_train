{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,ConfusionMatrixDisplay\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets (replace with your actual file paths)\n",
    "train_data = pd.read_csv(r'C:\\Users\\Harsh244635\\OneDrive - EXLService.com (I) Pvt. Ltd\\Documents\\Capstone Project\\dataset\\X_train_cleaned.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\Harsh244635\\OneDrive - EXLService.com (I) Pvt. Ltd\\Documents\\Capstone Project\\dataset\\X_train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and targets\n",
    "X_train_cleaned = train_data['review_body'].tolist()  # Text data\n",
    "Y_train = train_data['sentiment'].tolist()  # Labels\n",
    "X_test_cleaned = test_data['review_body'].tolist()\n",
    "Y_test = test_data['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is a list of strings\n",
    "X_train_cleaned = [str(item) for item in X_train_cleaned]\n",
    "X_test_cleaned = [str(item) for item in X_test_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train_cleaned, Y_train, test_size=0.2, random_state=42, stratify=Y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to pandas Series\n",
    "X_train = pd.Series(X_train)\n",
    "X_val = pd.Series(X_val)\n",
    "X_test = pd.Series(X_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    tried it doe not work i saw the blog of previo...\n",
      "1    i give this product mixed review in principle ...\n",
      "2    great product for the price i had some frayes ...\n",
      "3    first of all i got them for dollar and they su...\n",
      "4    i can t say much it never functioned and i wa ...\n",
      "dtype: object"
     ]
    }
   ],
   "source": [
    "# Define tokenizer and lemmatizer\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Define function to lemmatize the review text\n",
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for word in w_tokenizer.tokenize(text):\n",
    "        st += lemmatizer.lemmatize(word) + \" \"\n",
    "    return st.strip()  # Strip to remove any trailing spaces\n",
    "\n",
    "# Apply lemmatization to the 'review' text in the datasets\n",
    "X_train = X_train.apply(lemmatize_text)\n",
    "X_val = X_val.apply(lemmatize_text)\n",
    "X_test = X_test.apply(lemmatize_text)\n",
    "\n",
    "# Optionally, print a sample to check\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to ensure uniform input size\n",
    "max_length = 100  # You can change this based on your data\n",
    "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=max_length)\n",
    "X_val_pad = pad_sequences(X_val_seq, padding='post', maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical (one-hot encoding) for multi-class classification\n",
    "y_train = to_categorical(Y_train, num_classes=3)\n",
    "y_val = to_categorical(Y_val, num_classes=3)\n",
    "y_test = to_categorical(Y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh244635\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn("
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_length))\n",
    "\n",
    "# BiLSTM layer\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "# Output layer with softmax activation for multi-class classification\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
    "              loss='categorical_crossentropy',  # Suitable for multi-class classification\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8582/8582 - 387s - 45ms/step - accuracy: 0.4101 - loss: 1.0637 - val_accuracy: 0.5346 - val_loss: 0.9671\n",
      "Epoch 2/3\n",
      "8582/8582 - 412s - 48ms/step - accuracy: 0.4921 - loss: 0.9572 - val_accuracy: 0.6064 - val_loss: 0.8662\n",
      "Epoch 3/3\n",
      "8582/8582 - 425s - 49ms/step - accuracy: 0.5514 - loss: 0.8934 - val_accuracy: 0.6340 - val_loss: 0.8147\n",
      "10727/10727 - 163s - 15ms/step - accuracy: 0.6363 - loss: 0.8104\n",
      "Test accuracy: 0.6363"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=3,  # Number of epochs\n",
    "    batch_size=32,  # You can adjust based on your data\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluating the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. "
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('biLSTM_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,795</span> (15.80 MB)\n",
       "</pre>"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,142,795\u001b[0m (15.80 MB)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,380,931</span> (5.27 MB)\n",
       "</pre>"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,380,931\u001b[0m (5.27 MB)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,761,864</span> (10.54 MB)\n",
       "</pre>"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,761,864\u001b[0m (10.54 MB)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model."
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BiLSTM model\n",
    "model = tf.keras.models.load_model(r'C:\\Users\\Harsh244635\\OneDrive - EXLService.com (I) Pvt. Ltd\\Documents\\Capstone Project\\biLSTM_model_2.h5')\n",
    "\n",
    "# Define the maximum length of sequences (same as what was used during training)\n",
    "max_sequence_length = 100  # Adjust based on the training config\n",
    "\n",
    "# Recreate the Tokenizer used during training (assuming you don't have the original tokenizer)\n",
    "vocab_size = 10000  # You can adjust this if you know the vocab size used during training\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the input sentence\n",
    "def preprocess_text(sentences):\n",
    "    # Tokenize and pad the input sentences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "    \n",
    "    # Debugging: Print out the number of sentences after preprocessing\n",
    "    print(f\"Number of processed (padded) sentences: {len(padded_sequences)}\")\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment for a list of sentences\n",
    "def predict_sentiments(sentences):\n",
    "    processed_sentences = preprocess_text(sentences)\n",
    "    \n",
    "    # Debugging: Print out the shape of the processed sentences\n",
    "    print(f\"Shape of processed sentences: {processed_sentences.shape}\")\n",
    "    \n",
    "    # Predicting sentiment for the input sentences\n",
    "    predictions = model.predict(processed_sentences)\n",
    "    \n",
    "    # Debugging: Print out the shape of predictions\n",
    "    print(f\"Shape of predictions: {predictions.shape}\")\n",
    "    \n",
    "    # The prediction is a vector of probabilities for each class: [positive, negative, neutral]\n",
    "    sentiment_classes = np.argmax(predictions, axis=1)  # Get the index of the max probability for each sentence\n",
    "    \n",
    "    # Map the index to sentiment labels\n",
    "    sentiment_labels = ['Positive', 'Negative', 'Neutral']\n",
    "    predicted_labels = [sentiment_labels[i] for i in sentiment_classes]\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed (padded) sentences: 19\n",
      "Shape of processed sentences: (19, 100)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Shape of predictions: (19, 3)\n",
      "The sentiment of the sentence is: ['Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral']"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence = \"I love this product\"\n",
    "sentiment = predict_sentiments(sentence)\n",
    "print(f\"The sentiment of the sentence is: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test dataset (sentences and true labels)\n",
    "test_sentences = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst thing I have ever bought.\",\n",
    "    \"It's okay, neither good nor bad.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels for the test dataset (replace these with actual labels from your dataset)\n",
    "true_labels = ['Positive', 'Negative', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in test data: 3\n",
      "True labels: ['Positive', 'Negative', 'Neutral']"
     ]
    }
   ],
   "source": [
    "# Check the number of sentences\n",
    "print(f\"Number of sentences in test data: {len(test_sentences)}\")\n",
    "print(f\"True labels: {true_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed (padded) sentences: 3\n",
      "Shape of processed sentences: (3, 100)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Shape of predictions: (3, 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Generate predictions for the test dataset\n",
    "predictions = predict_sentiments(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure predictions and true labels are in a list format (if not already)\n",
    "predictions = np.array(predictions)  # Convert predictions to a NumPy array\n",
    "true_labels = np.array(true_labels)  # Convert true labels to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for given sentences:\n",
      "Sentence: 'I love this product!' => Predicted Sentiment: Negative\n",
      "Sentence: 'This is the worst thing I have ever bought.' => Predicted Sentiment: Neutral\n",
      "Sentence: 'It's okay, neither good nor bad.' => Predicted Sentiment: Neutral"
     ]
    }
   ],
   "source": [
    "# Print each sentence with its predicted sentiment\n",
    "print(\"Predictions for given sentences:\")\n",
    "for sentence, prediction in zip(test_sentences, predictions):\n",
    "    print(f\"Sentence: '{sentence}' => Predicted Sentiment: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 3\n",
      "Number of true labels: 3"
     ]
    }
   ],
   "source": [
    "# Check if the number of predictions matches the number of true labels\n",
    "print(f\"Number of predictions: {len(predictions)}\")\n",
    "print(f\"Number of true labels: {len(true_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.00      0.00      0.00         1\n",
      "    Negative       0.50      1.00      0.67         1\n",
      "     Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.33      0.22         3\n",
      "weighted avg       0.17      0.33      0.22         3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh244635\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Harsh244635\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Harsh244635\\AppData\\Local\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
     ]
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "print(\"Classification Report:\\)\n",
    "print(classification_report(true_labels, predictions, target_names=['Positive', 'Negative', 'Neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10727/10727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 32ms/step"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the saved BiLSTM model\n",
    "model = load_model('biLSTM_model_2.h5')\n",
    "\n",
    "# Load the test dataset (assuming it's in CSV format)\n",
    "test_data = pd.read_csv(r'C:\\Users\\Harsh244635\\OneDrive - EXLService.com (I) Pvt. Ltd\\Documents\\Capstone Project\\dataset\\X_train_cleaned.csv')\n",
    "\n",
    "# Extract test features and labels\n",
    "X_test_cleaned = test_data['review_body'].tolist()\n",
    "Y_test = test_data['sentiment'].tolist()\n",
    "\n",
    "# Ensure the data is a list of strings\n",
    "X_test_cleaned = [str(item) for item in X_test_cleaned]\n",
    "\n",
    "# Apply lemmatization to the test set (same function as during training)\n",
    "X_test = pd.Series(X_test_cleaned).apply(lemmatize_text)\n",
    "\n",
    "# Tokenize the test data using the tokenizer trained on the training set\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to the same length as the training data\n",
    "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=100)  # Use the same maxlen as during training\n",
    "\n",
    "# Make predictions using the trained model\n",
    "y_pred = model.predict(X_test_pad)\n",
    "\n",
    "# Convert probabilities to class labels (using np.argmax to get the class with the highest probability)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Encode the true labels (if they are in string format, like \"positive\", \"negative\", \"neutral\")\n",
    "# Convert to numeric labels if the labels are not already numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y_test_classes = label_encoder.fit_transform(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 1, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test_classes and y_pred_classes to lists\n",
    "y_test_classes_list = y_test_classes.tolist()\n",
    "y_pred_classes_list = y_pred_classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68    114418\n",
      "           1       0.53      0.30      0.39    114419\n",
      "           2       0.78      0.78      0.78    114419\n",
      "\n",
      "    accuracy                           0.64    343256\n",
      "   macro avg       0.63      0.64      0.62    343256\n",
      "weighted avg       0.63      0.64      0.62    343256\n"
     ]
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test_classes_list, y_pred_classes_list)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
